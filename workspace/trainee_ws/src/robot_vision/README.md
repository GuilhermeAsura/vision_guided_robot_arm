# Robot Vision & Control (ROS2 Package)

Este pacote implementa o sistema de percep√ß√£o e controle aut√¥nomo do manipulador UR5e. Ele √© respons√°vel por processar as imagens da c√¢mera simulada, extrair coordenadas de objetos de interesse (c√≠rculos vermelhos) e comandar o bra√ßo rob√≥tico utilizando tanto **Visual Servoing** (controle reativo) quanto **Cinem√°tica Inversa** (planejamento de trajet√≥ria).

## üéØ Funcionalidades Principais

1.  **Processamento de Imagem:** Detec√ß√£o de cores e c√°lculo de centroides usando OpenCV.
2.  **Visual Servoing:** Algoritmo de controle proporcional para alinhar a c√¢mera com o objeto.
3.  **Cinem√°tica Inversa (IK):** Uso da biblioteca `ikpy` para calcular os √¢ngulos das juntas a partir de coordenadas (x, y, z).
4.  **Automa√ß√£o (Pick & Place):** M√°quina de estados finita para executar a tarefa de pegar e levantar um objeto.

## üìÇ Estrutura do Pacote

```text
robot_vision/
‚îú‚îÄ‚îÄ launch/
‚îÇ   ‚îú‚îÄ‚îÄ complete_system.launch.py    # Lan√ßa todo o sistema (Vis√£o + Controle Manual)
‚îÇ   ‚îî‚îÄ‚îÄ visual_servoing.launch.py    # Lan√ßa o sistema de Vis√£o + Controle Aut√¥nomo
‚îú‚îÄ‚îÄ robot_vision/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ vision_node.py               # N√≥ de Percep√ß√£o (OpenCV)
‚îÇ   ‚îú‚îÄ‚îÄ visual_controller.py         # N√≥ de Autonomia (IK + M√°quina de Estados)
‚îÇ   ‚îî‚îÄ‚îÄ visual_servoing_controller.py # N√≥ de Rastreamento Simples (P-Controller)
‚îú‚îÄ‚îÄ package.xml
‚îî‚îÄ‚îÄ setup.py
````

## ‚öôÔ∏è Depend√™ncias

Al√©m das depend√™ncias padr√£o do ROS2 (`rclpy`, `std_msgs`, `geometry_msgs`), este pacote requer:

- **OpenCV (`python3-opencv`):** Para processamento de imagem.
    
- **CvBridge (`cv_bridge`):** Para converter mensagens ROS `sensor_msgs/Image` em arrays NumPy.
    
- **IKPy (`ikpy`):** Para c√°lculos de cinem√°tica inversa baseados em URDF.
    
- **NumPy:** Para √°lgebra linear.
    

## üß† N√≥s (Nodes) Detalhados

### 1. `vision_node.py` (O "Olho")

Este n√≥ atua puramente na camada de percep√ß√£o. Ele n√£o sabe o que √© um rob√¥, apenas processa imagens.

- **Assina:** `/UR5e/camera_sensor/image_color`
    
- **Publica:** `/vision/object_coordinates` (Tipo: `geometry_msgs/Point`)
    
- **Algoritmo:**
    
    1. Aplica desfoque (Gaussian Blur) para reduzir ru√≠do.
        
    2. Converte espa√ßo de cor BGR para **HSV**.
        
    3. Aplica m√°scaras para filtrar a cor vermelha.
        
    4. Encontra contornos e calcula o **Momento** da imagem para achar o centroide (X, Y).
        

### 2. `visual_servoing_controller.py` (O "Seguidor")

Um controlador h√≠brido que implementa um comportamento de "olhar para o objeto".

- **L√≥gica:** Recebe as coordenadas do `vision_node` e calcula o **Erro** (dist√¢ncia do objeto ao centro da imagem).
    
- **Controle:** Aplica um ganho proporcional ($P$) para mover a Base e o Ombro do rob√¥, tentando zerar o erro (centralizar o objeto na imagem).
    
- **Interatividade:** Pressione a tecla `T` para ativar/desativar o rastreamento (Tracking).
    

### 3. `visual_controller.py` (O "Bra√ßo Aut√¥nomo")

Este √© o script mais avan√ßado, respons√°vel pelo _Pick and Place_.

- **Cinem√°tica Inversa:** Carrega o URDF do rob√¥ e utiliza `ikpy` para calcular os √¢ngulos necess√°rios para atingir uma posi√ß√£o (X, Y, Z) com a garra apontada para baixo.
    
- **M√°quina de Estados:**
    
    - `SEARCH`: Procura o alvo.
        
    - `APPROACH`: Posiciona-se acima do objeto.
        
    - `LOWER`: Desce at√© a altura de pega.
        
    - `GRASP`: Fecha a garra.
        
    - `LIFT`: Levanta o objeto.
        
- **Ground Truth:** Para garantir a precis√£o do "agarre" f√≠sico, este n√≥ utiliza o `Supervisor` do Webots para obter a posi√ß√£o 3D absoluta do objeto, enquanto a c√¢mera √© usada para monitoramento visual.
    

## üöÄ Como Executar

### Cen√°rio 1: Sistema Completo (Manual)

Roda a vis√£o computacional e permite controle pelo teclado.


``` Bash
ros2 launch robot_vision complete_system.launch.py
```

### Cen√°rio 2: Autonomia (Pick & Place ou Servoing)

Roda o n√≥ de vis√£o junto com o controlador aut√¥nomo escolhido.


``` Bash
ros2 launch robot_vision visual_servoing.launch.py
```

## üõ†Ô∏è Destaques T√©cnicos

- **Desacoplamento:** A vis√£o computacional roda em um processo separado, publicando coordenadas gen√©ricas. Isso permite trocar o algoritmo de vis√£o sem quebrar o controle do rob√¥.
    
- **Tratamento de Cores:** O uso de HSV em vez de RGB torna a detec√ß√£o mais robusta a mudan√ßas de ilumina√ß√£o.
    
- **Seguran√ßa:** O controlador aut√¥nomo verifica limites de juntas e utiliza m√°scaras de links no IKPy para garantir movimentos suaves.

### Abordagem IBVS - Image-Based Visual Servoing

Adotamos uma arquitetura de Visual Servoing Baseado em Imagem **(IBVS - Image-Based Visual Servoing)** com uma abordagem heur√≠stica direta.<br>

(_"Servoing"_ refere-se √† t√©cnica de controle de movimento de um rob√¥ usando feedback visual extra√≠do de uma c√¢mera.)

  

**1. Eliminamos a Cinem√°tica Inversa (IK)**:

Na rob√≥tica cl√°ssica, o fluxo seria:

Detectar objeto em pixels `(u, v)`; converter pixels para coordenadas 3D no mundo `(x, y, z)` usando a matriz intr√≠nseca da c√¢mera e profundidade; calcular a Cinem√°tica Inversa para descobrir quais √¢ngulos de junta `(Œ∏1, Œ∏2, ...)` levam o efetuador at√© `(x, y, z)`; mover para esses √¢ngulos.<br>

<br>

N√≥s sabemos que se o objeto est√° √† esquerda na imagem, precisamos girar a Base para a esquerda. Sabemos que se o objeto est√° em cima na imagem, precisamos levantar o Ombro. Assim, mapeamos o **Erro em Pixels** diretamente para **Velocidade da Junta**, sem passar pela matem√°tica complexa de coordenadas cartesianas 3D.

<br>

  

**2. Utilizamos um Controlador Proporcional (P-Controller)**:

<br>

A l√≥gica matem√°tica se resume a: `Velocidade = Ganho x Erro`

- **Erro (e)**: Ele calcula a diferen√ßa entre onde o objeto est√° `(x, y)` e o centro da imagem (320, 240).

- **Lei de Controle**: _`Vjunta = Kp x e`_

Se o objeto est√° √† direita (erro positivo), movemos a junta positivamente.

Se o erro √© zero (centralizado), a velocidade √© zero.

- **Loop Principal**: A cada passo da simula√ß√£o (step), lemos os bytes da c√¢mera e empacotamos numa mensagem ROS padr√£o (bgra8 √© o padr√£o do Webots, o cv_bridge no outro n√≥ far√° a convers√£o autom√°tica).

  

Dessa forma o **n√≥ de controle** tem duas responsabilidades:

<br>

**Output (Atuadores)**: Receber comandos e mover juntas (j√° implementado).<br>

**Input (Sensores)**: Ler a c√¢mera do Webots e publicar a imagem bruta para o ROS.

<br>

N√£o precisamos de integrais ou derivadas porque o loop de controle roda muito r√°pido (32ms a 60ms). O rob√¥ faz corre√ß√µes min√∫sculas e cont√≠nuas. Se ele n√£o chegar l√° na primeira tentativa, o loop roda de novo e ele corrige mais um pouco. Isso remove a necessidade de planejamento de trajet√≥ria complexo (Splines, Curvas de Bezier).<br>  

Simplificamos o problema de um sistema **MIMO (M√∫ltiplas Entradas, M√∫ltiplas Sa√≠das)** para dois sistemas simples **SISO (Entrada √önica e Sa√≠da √önica)**, onde o erro em X controla apenas a Junta 0 e o erro em Y controla apenas a Junta 1.<br>

Se fossemos usar a Matriz Jacobiana de Imagem (a forma tradicional), o c√≥digo teria que calcular matrizes 2x6, inverte-las e fazer multiplica√ß√£o matricial a cada frame. Nossa abordagem heur√≠stica funciona perfeitamente para centralizar objetos sem essa sobrecarga computacional. A l√≥gica diminuiu porque trocamos um **c√°lculo geom√©trico expl√≠cito** (pesado e extenso) por **controle reativo em malha fechada** (leve e iterativo). O rob√¥ n√£o "sabe" onde o objeto est√° no espa√ßo 3D, ele apenas sabe que precisa reduzir o erro na imagem para zero.